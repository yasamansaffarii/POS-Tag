# -*- coding: utf-8 -*-
"""POSfarsi_CNN_BILSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TfILgxnWG2U4DYOFZUbaaQ9ESDTryJI8
"""

import pandas as pd
import numpy as np

#خواندن دو فایل داده و ترکیبشن در یک فایل جدید

import csv
reader = csv.reader(open("/content/data.csv"))
reader1 = csv.reader(open("/content/data2.csv"))
f = open("/content/data1.csv", "w")
writer = csv.writer(f)

for row in reader:
    writer.writerow(row)
for row in reader1:
    writer.writerow(row)
f.close()

#خواندن فایل داده جدید که ازین پس باهاش کار داریم

data = pd.read_csv('/content/data1.csv' , encoding='UTF-8')
#data = pd.read_csv('/content/data.csv' , encoding='UTF-8')
#data2 = pd.read_csv('/content/data2.csv' , encoding='UTF-8')
#print(data1.size)
#print(data2.size)
#print(data.size)

data.dtypes

#مطمین شم درایه خالی ندارم تو دیتاستم
data.isna().sum()

#تعداد کلمات یونیک و تگ ها

print((data["word"]).nunique())
print((data["Pred. Tag"]).nunique())

#ستون کلمات یونیکو برمیدارم و یک لیست میکنم و بهش یه کلمه جدید به عنوان پد اخر جمله اضافه میکنم 

words=list(set(data["word"].values))
words.append("ENDPAD")
num_words=len(words)

#تگ های منحصر به فردو برمیدارم و یک لیست میکنم

tags=list(set(data["Pred. Tag"].values))
num_tags=len(tags)

print(tags)

num_words,num_tags

#دیتاستمو باز میکنم و یک لیست از تاپلهای هر کلمه و تگشو میسازم. برای همه کلمات یونیک و غیر یونیک. و جملات رو بر اساس نقطه جدا میکنم و لیست جملات رو میسازم.

from csv import reader

with open('/content/data1.csv', 'r') as read_obj:
    # pass the file object to reader() to get the reader object
    csv_reader = reader(read_obj)
    # Get all rows of csv from csv_reader object as list of tuples
    list_of_tuples = list(map(tuple, csv_reader))

wordsnew=list_of_tuples[1:]

import itertools
from more_itertools import split_after
sentences=list(split_after(wordsnew, lambda x: x == ('.', 'PUNC') ))

print(sentences[0])

#دو تا عملیات مهم که بعدا هم استفاده میشن. کلمات و تگ ها رو بر حسب ایندکس یونیک شماره گزاری میکنه که در بردار وان هات کاربرد داره. در برگردوندن وان هات به کلمه ی تگ هم کاربرد داره.

word2idx = {w: i for i, w in enumerate(words)}
tag2idx = {t: i for i, t in enumerate(tags)}

tag2idx

#تمام کلمات همه جملات دیتاستو ایندکس گذاری میکنه و بر اساس یک ماکسیمم طولی جملات رو پد میزاره اخرشون تا طولشون ثابت باشه برای ورود به شبکه
#همینکارو برای تگهای مربوطه هم میکنه و چون که تعداد تگها فیکسه همینجا با کتگوریکال وان هاتش میکنه.
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

max_len=50

X = [[word2idx[w[0]] for w in s] for s in sentences]
X = pad_sequences(maxlen=max_len, sequences=X, padding="post", value=num_words - 1)

y = [[tag2idx[w[1]] for w in s] for s in sentences]
y = pad_sequences(maxlen=max_len, sequences=y, padding="post", value=tag2idx["H1"])
y=[to_categorical(i, num_classes=num_tags) for i in y]
print(y[0])

#درست کردن مجموعه ترین و تست از کل دیتاست
#دقت کنید که در مدل عصبی باید تنسورها ارایه باشند ولی اینجا تگها لیستند که در ادامه همه جا به ارایه تبدیل میشه
from sklearn.model_selection import train_test_split
 
 
(X_train, 
 X_test, 
 y_train, 
 y_test) = train_test_split(X, y, test_size=0.1, random_state=1)
#X_train.shape
type(y)

print(y_test[0])

from tensorflow.keras import  Model,Input
from tensorflow.keras.layers import LSTM, Embedding, Dense, Conv1D,MaxPooling1D, UpSampling1D, concatenate
#from keras.layers.convolutional import Deconvolution1D
from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional
from sklearn.model_selection import KFold
from tensorflow.keras.models import save_model, load_model

#pip install livelossplot

from tensorflow.keras.callbacks import  ModelCheckpoint, EarlyStopping
#from livelossplot.tf_keras import PlotLossesCallback
import gc
gc.collect(),gc.collect()

#cnn_bilstm
#input_word=Input(shape=(max_len,))

input_word=Input(shape=(max_len,))
emb=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model1=SpatialDropout1D(0.3)(emb)
model1=Conv1D(filters=24, kernel_size=3, padding='same', activation='relu')(model1)
model1=MaxPooling1D(pool_size=2)(model1)

#input_word2=Input(shape=(max_len,))
#model2=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model2=Conv1D(filters=24, kernel_size=4, padding='same', activation='relu')(emb)
model2=MaxPooling1D(pool_size=2)(model2)

#input_word3=Input(shape=(max_len,))
#model3=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model3=Conv1D(filters=24, kernel_size=5, padding='same', activation='relu')(emb)
model3=MaxPooling1D(pool_size=2)(model3)

model4 = concatenate([model1,model2,model3],axis = -1)
model4 = UpSampling1D(size=2)(model4)
model4=Bidirectional(LSTM(units=100,return_sequences=True, recurrent_dropout=0.1))(model4)
model4=TimeDistributed(Dense(23,activation='relu'))(model4)
out=TimeDistributed(Dense(num_tags,activation='softmax'))(model4)
model=Model(input_word,out)
model.summary()

#bilstm_cnn
#input_word=Input(shape=(max_len,))

input_word=Input(shape=(max_len,))
emb=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model1=SpatialDropout1D(0.3)(emb)
model1=Bidirectional(LSTM(units=100,return_sequences=True, recurrent_dropout=0.1))(model1)
model2=Conv1D(filters=24, kernel_size=3, padding='same', activation='relu')(model1)
model2=MaxPooling1D(pool_size=2)(model2)

#input_word2=Input(shape=(max_len,))
#model2=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model3=Conv1D(filters=24, kernel_size=4, padding='same', activation='relu')(model1)
model3=MaxPooling1D(pool_size=2)(model3)

#input_word3=Input(shape=(max_len,))
#model3=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model4=Conv1D(filters=24, kernel_size=5, padding='same', activation='relu')(model1)
model4=MaxPooling1D(pool_size=2)(model4)

model5 = concatenate([model2,model3,model4],axis = -1)
model5 = UpSampling1D(size=2)(model5)
model5=TimeDistributed(Dense(23,activation='relu'))(model5)
out=TimeDistributed(Dense(num_tags,activation='softmax'))(model5)
model=Model(input_word,out)
model.summary()

#cnn
#input_word=Input(shape=(max_len,))

input_word=Input(shape=(max_len,))
emb=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model1=SpatialDropout1D(0.3)(emb)
model1=Conv1D(filters=24, kernel_size=3, padding='same', activation='relu')(model1)
model1=MaxPooling1D(pool_size=2)(model1)

#input_word2=Input(shape=(max_len,))
#model2=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model2=Conv1D(filters=24, kernel_size=4, padding='same', activation='relu')(emb)
model2=MaxPooling1D(pool_size=2)(model2)

#input_word3=Input(shape=(max_len,))
#model3=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model3=Conv1D(filters=24, kernel_size=5, padding='same', activation='relu')(emb)
model3=MaxPooling1D(pool_size=2)(model3)

model4 = concatenate([model1,model2,model3],axis = -1)
model4 = UpSampling1D(size=2)(model4)
model4=TimeDistributed(Dense(23,activation='relu'))(model4)
out=TimeDistributed(Dense(num_tags,activation='softmax'))(model4)
model=Model(input_word,out)
model.summary()

#bilstm
#cnn_bilstm
#input_word=Input(shape=(max_len,))

input_word=Input(shape=(max_len,))
emb=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model1=SpatialDropout1D(0.3)(emb)
model1=Bidirectional(LSTM(units=100,return_sequences=True, recurrent_dropout=0.1))(model1)
model1=TimeDistributed(Dense(23,activation='relu'))(model1)
out=TimeDistributed(Dense(num_tags,activation='softmax'))(model1)
model=Model(input_word,out)
model.summary()

#emb in bilstmبهترین نتیجه 
#input_word=Input(shape=(max_len,))

input_word=Input(shape=(max_len,))
emb=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model1=SpatialDropout1D(0.3)(emb)
model1=Conv1D(filters=24, kernel_size=3, padding='same', activation='relu')(model1)
model1=MaxPooling1D(pool_size=2)(model1)

#input_word2=Input(shape=(max_len,))
#model2=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model2=Conv1D(filters=24, kernel_size=4, padding='same', activation='relu')(emb)
model2=MaxPooling1D(pool_size=2)(model2)

#input_word3=Input(shape=(max_len,))
#model3=Embedding(input_dim=num_words, output_dim= 32, input_length=max_len)(input_word)
model3=Conv1D(filters=24, kernel_size=5, padding='same', activation='relu')(emb)
model3=MaxPooling1D(pool_size=2)(model3)

model4 = concatenate([model1,model2,model3],axis = -1)
model4 = UpSampling1D(size=2)(model4)
model5=concatenate([model4, emb], axis=-1)
model5=Bidirectional(LSTM(units=100,return_sequences=True, recurrent_dropout=0.1))(model5)
model5=TimeDistributed(Dense(23,activation='relu'))(model5)
out=TimeDistributed(Dense(num_tags,activation='softmax'))(model5)
model=Model(input_word,out)
model.summary()

#چون که تو پایتون جدید متریک های ریکال اینا حذف شده توابعشونو مینویسیم و ازینا استفاده میکنیم.

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',f1_m,precision_m, recall_m])

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
early_stopping=EarlyStopping(monitor="val_accuracy", verbose=0,mode="max", restore_best_weights= False)
callbacks=[early_stopping]

history=model.fit(X_train, np.array(y_train),validation_split=0.1, batch_size=20, epochs=10
                  , verbose=1)

# مدلمونو ذخیره میکنیم 
filepath = '/content/saved_model'
save_model(model, filepath)

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#تست مدلمون به صورت دستی
#این دانلود پکیج فراموش نشود
nltk.download('punkt')
import nltk
from nltk.tokenize import word_tokenize
from keras.preprocessing.text import Tokenizer as tokenizer

# function to return key for any value
def get_key(val):
	for key, value in tag2idx.items():
		if val == value:
			return key


print("یک جمله میگیریم و توکنایز میکنیم************")
new_complaint = 'بیا بریم دشت'
X= word_tokenize(new_complaint)
print(X)
print("کلمات جمله رو ایندکس گذاری و پد میزنیم تا طول 50 برای یک جمله*********************")
X = [word2idx[w] for w in X] 
print(X)
X = pad_sequences(maxlen=max_len, sequences=[X], padding="post", value=num_words - 1)
print(X)
print("پیشگویی*************")
pred = model.predict(X)
print("طول آرایه حاصل از پردیکت جمله ورودی  ",len(pred))
#ازونجایی که کلا یک جمله داریم پسهمیشه در اینجا درایه اول صفر هست و درایه دوم نشانگر کلمه
print('بردار پیشبینی شده برای جمله صفرم و کلمه صفرم',pred[0,0])
print(' ایندکس تگ مربوط به جمله صفرم و کلمه صفرم',np.argmax(pred[0,0]))





print("پردیکت روی مجموعه تست********")
pred = model.predict(X_test)

print("پیشبینی تگ جمله هشتادم مجموعه تست و کلمه دوم",pred[80,2])
print("ایندکس پیشبینی تگ مربوط به کلمه دوم جمله هشتادم مجموعه تست: ",np.argmax(pred[80,2]))
print("تگ پیشبینی شده مربوط به کلمه دوم جمله هشتادم از مجموعه تست",get_key(np.argmax(pred[80,2])))
print("کد واقعی وان هات تگ مربوط به کلمه دوم جمله هشتادم مجموعه تست",(np.array(y_test))[80,2])

#جملات جدید بدهید و برچسب اجزای سخن کلمات جمله را دریافت کنید.
import nltk
from nltk.tokenize import word_tokenize
#nltk.download('punkt')
from keras.preprocessing.text import Tokenizer as tokenizer
new_complaint = ' زندگی آسان است.'

def get_key(val):
	for key, value in tag2idx.items():
		if val == value:
			return key
def predict_sent(new_complaint):
  X= word_tokenize(new_complaint)
  print(X)
  X = [word2idx[w] for w in X] 
  print(X)
  X = pad_sequences(maxlen=max_len, sequences=[X], padding="post", value=num_words - 1)
  print(X)
  pred = model.predict(X)
  for w in range(len(pred[0])):
    print(get_key(np.argmax(pred[0,w])))

predict_sent(new_complaint)

#سعی دارم میکنم ماتریس درهم امیختگی بسازم که همش به خاطر مولتی کلاس بودن خطا میگیرم

import sklearn
y_pred = model.predict(X_test)
confusion_matrix = sklearn.metrics.confusion_matrix(np.array(y_test), np.rint(y_pred))

import seaborn as sns
from sklearn.metrics import multilabel_confusion_matrix
pred = model.predict(X_test)
cm = multilabel_confusion_matrix(np.array(y_test), pred)
f = sns.heatmap(cm, annot=True)

y_test_non_category = [ np.argmax(t) for t in y_test ]
y_predict_non_category = [ np.argmax(t) for t in pred ]

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test_non_category, y_predict_non_category)
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(keras_estimator,X_test, y_test_non_category,labels =sorted(tag2idx, key=tag2idx.get), normalize=False)  
plt.show()

def estimator(X):
  pred = model.predict(X)
  for w in range(len(pred[0])):
    print(get_key(np.argmax(pred[0,w]))
  pred=model.predict(X)
  y_predict_non_category = [ np.argmax(t) for t in pred ]
  return y_predict_non_category

import tempfile
import tensorflow as tf
model_dir ='/content/saved_model'
keras_estimator = tf.keras.estimator.model_to_estimator(
    keras_model=model, model_dir=model_dir)

# میتونیم نتیجه ارزیابیو بخشهاشو جداگانه بریزیم تو متغیر که حالا فعلا استفاده نمیکنم
loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, np.array(y_test), verbose=0)

# ارزیابی مدل روی مجموعه تست
print(len(y_train[0]))
score = model.evaluate(X_test, np.array(y_test), verbose=1)
print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')